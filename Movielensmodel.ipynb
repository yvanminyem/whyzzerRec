{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movielensmodel.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvanminyem/whyzzerRec/blob/main/Movielensmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChjuaQjm_iBf"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikhIvrku-i-L"
      },
      "source": [
        "# Building deep retrieval models\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/deep_recommenders\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWqCArLO_kez"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7RYXwgbAcbU"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "We first import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgFBaQZEbw3O"
      },
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbwMjnLP5nZ_"
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from keras.models import load_model\n",
        "\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgKIjpQLAiax"
      },
      "source": [
        "Loading the data and splitting into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc2REbOO52Fl"
      },
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZ2q5RXYNI6"
      },
      "source": [
        "We also do some housekeeping to prepare feature vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5CVveCS9Doq"
      },
      "source": [
        "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
        "\n",
        "max_timestamp = timestamps.max()\n",
        "min_timestamp = timestamps.min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000,\n",
        ")\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJcCVMUQou3"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ItzYwMW42cb"
      },
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=unique_user_ids, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
        "    ])\n",
        "    self.timestamp_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
        "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
        "    ])\n",
        "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
        "        axis=None\n",
        "    )\n",
        "\n",
        "    self.normalized_timestamp.adapt(timestamps)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Take the input dictionary, pass it through each input layer,\n",
        "    # and concatenate the result.\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojgHM8F2seF-"
      },
      "source": [
        "now adding layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qfPi4I-Z0ph"
      },
      "source": [
        "class QueryModel(tf.keras.Model):\n",
        "  \"\"\"Model for encoding user queries.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    \"\"\"Model for encoding user queries.\n",
        "\n",
        "    Args:\n",
        "      layer_sizes:\n",
        "        A list of integers where the i-th entry represents the number of units\n",
        "        the i-th layer contains.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # We first use the user model for generating embeddings.\n",
        "    self.embedding_model = UserModel()\n",
        "\n",
        "    # Then construct the layers.\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "    # Use the ReLU activation for all but the last layer.\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    # No activation for the last layer.\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9IqNTLmpJzs"
      },
      "source": [
        "The `layer_sizes` parameter gives us the depth and width of the model. We can vary it to experiment with shallower or deeper models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XleMceZNHC__"
      },
      "source": [
        "### Candidate model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZHX8bEHPOk"
      },
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.title_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "          vocabulary=unique_movie_titles,mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=max_tokens)\n",
        "\n",
        "    self.title_text_embedding = tf.keras.Sequential([\n",
        "      self.title_vectorizer,\n",
        "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer.adapt(movies)\n",
        "\n",
        "  def call(self, titles):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(titles),\n",
        "        self.title_text_embedding(titles),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6vssqPYp-gY"
      },
      "source": [
        "And expand it with hidden layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1gTXkvQqHGA"
      },
      "source": [
        "class CandidateModel(tf.keras.Model):\n",
        "  \"\"\"Model for encoding movies.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    \"\"\"Model for encoding movies.\n",
        "\n",
        "    Args:\n",
        "      layer_sizes:\n",
        "        A list of integers where the i-th entry represents the number of units\n",
        "        the i-th layer contains.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_model = MovieModel()\n",
        "\n",
        "    # Then construct the layers.\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "    # Use the ReLU activation for all but the last layer.\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    # No activation for the last layer.\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc4KbTNwHSvD"
      },
      "source": [
        "### Combined model\n",
        "\n",
        "With both `QueryModel` and `CandidateModel` defined, we can put together a combined model and implement our loss and metrics logic. To make things simple, we'll enforce that the model structure is the same across the query and candidate models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_hNJPKIh4-"
      },
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    super().__init__()\n",
        "    self.query_model = QueryModel(layer_sizes)\n",
        "    self.candidate_model = CandidateModel(layer_sizes)\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.candidate_model),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    # We only pass the user id and timestamp features into the query model. This\n",
        "    # is to ensure that the training inputs would have the same keys as the\n",
        "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
        "    # error when loading the query model after saving it.\n",
        "    query_embeddings = self.query_model({\n",
        "        \"user_id\": features[\"user_id\"],\n",
        "        \"timestamp\": features[\"timestamp\"],\n",
        "    })\n",
        "    movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
        "\n",
        "    return self.task(\n",
        "        query_embeddings, movie_embeddings, compute_metrics=not training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YXjsRsLTVzt"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7MTwMruoKh"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We first split the data into a training set and a testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMFUZ4dyTdYd"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(2048)\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2HEuTBzJ9w5"
      },
      "source": [
        "### Shallow model\n",
        "\n",
        "We're ready to try out our first, shallow, model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkoLkiQdK4Um"
      },
      "source": [
        "num_epochs = 100\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJ1anzuLXgN"
      },
      "source": [
        "### Deep model\n",
        "\n",
        "deep model with two layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qAr5gGMUxE"
      },
      "source": [
        "model = MovielensModel([64, 32])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "two_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYJC32HutJmQ"
      },
      "source": [
        "We can plot the validation accuracy curves to illustrate this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzriiDRlHEvo"
      },
      "source": [
        "num_validation_runs = len(two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"])\n",
        "\n",
        "epochs = [(x + 1)* 5 for x in range(num_validation_runs)]\n",
        "\n",
        "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
        "plt.title(\"Accuracy vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Top-100 accuracy\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwXJ3QMtte0j"
      },
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqoF8e4d3fXC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDmzV8lJtiIM"
      },
      "source": [
        "make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi06e10DtnLy"
      },
      "source": [
        "# Create a model that takes in raw query features, and\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
        "# recommends movies out of the entire movies dataset.\n",
        "index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.candidate_model)))\n",
        ")\n",
        "# Get recommendations.\n",
        "_, titles = index({\"user_id\": tf.constant([\"42\"]), \"timestamp\": tf.constant([12312431])})\n",
        "print(f\"Recommendations for user 138: {titles[0,:5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54R8hbiQtp7w"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWOprWNF7Pxr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsHdItGd8OyW"
      },
      "source": [
        "index.retrieval_task = tfrs.tasks.Retrieval()  # Removes the metrics.\n",
        "index.compile()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaAAgaXBTFvK"
      },
      "source": [
        "t = time.time()\n",
        "\n",
        "export_path_sm = \"./{}\".format(int(t))\n",
        "print(export_path_sm)\n",
        "\n",
        "\n",
        "tf.saved_model.save(index, export_path_sm)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnJgLJJe53D9"
      },
      "source": [
        "index.save_weights(\"/content/drive/MyDrive/Colab Notebooks/1629818214/ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hpfTyPTtvYb"
      },
      "source": [
        "!ls {export_path_sm}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kMSHNHut2iZ"
      },
      "source": [
        "loading the saved model as keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-gPFh3Pt4YY"
      },
      "source": [
        "\n",
        "reloaded_sm = tf.saved_model.load(export_path_sm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcmoyTSQt_MB"
      },
      "source": [
        "Download model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_5oocl0uBFl"
      },
      "source": [
        "!zip -r model.zip {export_path_sm}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdOvLGrbuENl"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tWJ0u2guGNS"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('./model.zip')\n",
        "except ImportError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}